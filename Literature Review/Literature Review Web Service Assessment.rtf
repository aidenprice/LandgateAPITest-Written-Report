{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf110
{\fonttbl\f0\fnil\fcharset0 OpenSans;\f1\fnil\fcharset0 OpenSans-Semibold;}
{\colortbl;\red255\green255\blue255;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
{\info
{\author Aiden Price}
{\*\company Curtin University}}\paperw11900\paperh16840\margl1440\margr1440\vieww38200\viewh24000\viewkind0
\deftab720
\pard\tx566\pardeftab720\sl276\slmult1\sa400

\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
\

\b\fs32 Web Service Assessment\

\f1\fs28 Web Service Evaluation
\f0\fs32 \

\b0\fs28 INTRODUCE\
Hamas, Saad and Abed \{*Hamad:2010tr\} compare the performance of SOAP and ReST APIs on mobile devices.  The measured criteria are response time and transmission size which predictably favour ReST interfaces.\
\pard\tx566\pardeftab720\sl276\slmult1\sa400\partightenfactor0
\cf0 Their experiment design emulates a mobile device on a desktop computer, further they restrict the simulated mobile network speed. These are useful controls in an experiment designed with a very clear aim of finding which service is faster. But real world complications such as heavy network traffic or poor signal are not addressed as a factor in the outcome. As an example, SOAP\'92s WS-ReliableMessaging protocol may reduce overall transfer time in areas with poor signal by minimising the number of failed message attempts.\
\pard\tx566\pardeftab720\sl276\slmult1\sa400
\cf0 Tian et al \{*Tian:2004cb\} design a server-client system that can optionally compress responses to save the client\'92s download limit or skip compression when the server is under heavy load to minimise timed out requests.\
Working in the pre-smartphone era the team simulated an iPAQ Pocket PC, emulating the device on a Pentium III laptop. The laptop emulating the client is connected to the server via Wifi, Bluetooth or a simulated mobile network. In order to simulate the increased latency and slower connection speed of a GPRS network they introduce another server that throttles network speed by artificially delaying messages. \
Davis, Kimo and Duarte-Figueiredo \{*Davis:2009hf\} focus on OGC Web Map Service (WMS) optimisation for mobile devices. They elaborate a service that combines the multi-layer composition of WMS with the mobile device response speed of AJAX-based web maps such as Google Maps.\
Their experiment implemented the proposed service and interacted with it from a custom application deployed on a Nokia N95. Given the focus on minimising data sent and received from the device the results vindicated their hypothesis. Unfortunately, the team declined to study response time results due to \'93severe fluctuations\'94 that they attributed to an overcrowded network. They conclude, by extension not experiment, that smaller volumes of transmitted data would result in faster map interaction overall.\
Fowler and Peterson \{*Fowler:2012bn\} built a custom iPhone application to test the performance of SOAP and ReST versions of a public transportation web service in Hamburg over a normal working day. They measured response time, data serialisation/deserialisation time and response size on the device itself and returned the results over their own bespoke web service. Simple and detailed messages of significantly different response size control whether response time is dependant upon message size. The results, as is common, are given as mean and standard deviation, descriptive statistics without discussion of error responses.\
Fowler and Peterson\'92s methodology called for the mobile user to remain \'93fixed\'94 while requesting and receiving the response, which we interpret as stationary. This is somewhat contrived behaviour for mobile device use. There are countless situations in which a mobile user would be active and moving while concurrently requesting data from a web service.\
Provisioning web services from a mobile device faces similar network and device limitations as does consuming a service from a mobile device. Nguyen, J\'f8rstad and van Thanh \{*Nguyen:2008jt\} explored web service performance on an emulated mobile device. While investigating the influence of varied simulated mobile network speeds they concluded that testing on an actual device would provide ideal settings for their network simulation. Indeed the subsequent experiment showed considerable differences between emulated and real network speed influence on web service performance. Even after modifying their simulated network speed to approximate real world network speed the difference is significant. \
Hussain, Wang and Toure \{*Hussain:2014ce\} test the response time and throughput of a variety of real world web services over DSL, Wifi and LAN internet connections. Results are simplified to descriptive statistics, average, minimum and maximum response time.\
Hussain, Wang and Toure discuss some tests with unusually long response times and speculate that it may be due to other web traffic. The specific time or other conditions of these particular tests are not elucidated. In fact, the methodology is not sufficiently detailed to provide all the parameters used in web service requests. Repeating their experiments will likely produce different results.\
Yang, Cao and Evans \{*Yang:2013ff\} demonstrate that WMS servers struggle with heavy loads of simultaneous requests. They  recorded response times to 1, 5, 10. 30, 50 and 100 concurrent requests to six important WMS servers. They found that response times increased with the number of requests until many servers either blocked all incoming requests to handle the load or simply timed out. They make number of recommendations, particularly regarding parallel requests and processing. Most helpfully to this study, a simple progress bar to indicate to a user that their request is still being attended to.
\f1\b \

\f0\b0 The emergence of ReSTful web services engendered a number of studies comparing performance to entrenched SOAP services. Few experimental designs take advantage of SOAP\'92s inherent advantages, i.e. a message layer and orchestrated distributed computing, such a design would allow SOAP APIs to be compared more favourably with ReST APIs.\
Castillo et al \{*Castillo:2011ve\} compare ReST and SOAP service implementations as the intermediary messaging layer for a genetic algorithm and a fitness evaluator. They also present one of the few papers to elaborate the advantages of the older SOAP API standard against a ReST API, but their experiment doesn\'92t build on this. Their proof of concept methodology introduces a useful control, requests via SOAP and ReST send strings of either 100 or 1,000 characters. The proportional time difference between large and small requests controls whether the response time depends upon the amount of data sent and received, illuminating how much response time overhead is due to the API employed. \
As other researchers, Castillo et al relate the results of their performance tests as average response time with a margin of error. They discuss accuracy, but this is in terms of their genetic algorithm\'92s accuracy, not the error rate of the web service API.\
\pard\tx566\pardeftab720\sl276\slmult1\sa400\partightenfactor0
\cf0 Kanagasundaram et al \{*Kanagasundaram:2012wv\} expose a student database as a resource and perform Create, Read, Update Delete operations over SOAP and ReST web services. The comparison of response time between different operations leads the team to propose a hybrid SOAP and ReST web service that incorporates the security and reliability aspects of SOAP with ReST APIs\'92 ease of development.\
The experiment design places the client and server processes on separate cores of a single processor. This works well as an experimental control but comes at the expense of measuring real world performance. Furthermore, the results obtained by Kanagasundaram et al \{*Kanagasundaram:2012wv\} are averaged response times from experiments repeated until they achieved a 95% confidence level. But not all are presented in the paper, only a select subset. Presumably messages resulting in errors were excluded from the averaged results. \
\pard\tx566\pardeftab720\sl276\slmult1\sa400\partightenfactor0
\cf0 Expedients such as simulated mobile networks or emulated mobile devices can be more easily understood when considered in the context of testing a production server. Stress testing and automated test case generation for example both benefit from a quantity of tests unachievable with a few handheld devices. Services such as Load Impact \{Anonymous:0z5u-mT-\} emulate thousands of mobile users on a variety of devices with a range of network connectivity.\
Mobile application testing is a rapidly growing field. Gao et al \{*Gao:2014fp\} identified four classes of mobile application testing approaches. Emulation-based testing, as seen above, where a device simulator runs on a desktop computer, is commonplace as emulators ship with most app development environments. Device-based testing runs the application and its services on a range of real devices to find edge case failures. Cloud testing suits large scale tests scaleable to requirements. Crowd-based testing employs volunteers or contractors to test apps and can reach reasonable scales, they note that tests may not be completely thorough.\
\pard\tx566\pardeftab720\sl276\slmult1\sa400\partightenfactor0
\cf0 Yan et al \{*Yan:2012uf\} built a cloud testing suite, an approach they called Web Service - Testing-as-a-Service (WS-TaaS). Their experiment showed that cloud tested web services were capable of responding to significantly more requests than one tested from a single desktop computer. Most likely due to the limited bandwidth and processing power of the single testing node.\
\pard\tx566\pardeftab720\sl276\slmult1\sa400

\f1\b \cf0 Web Service Quality and Discovery\

\f0\b0 The OASIS Web Services Quality Factors \{Kim:2012wm\} defines six quality factors with 28 sub-categories. \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl276\slmult1\sa400
\ls1\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	1.	}Business Value Quality - the value arising from using a web service as compared to the cost.\
{\listtext	2.	}Service Level Measurement Quality - the service responsiveness from a client\'92s point of view, including time and success criteria.\
{\listtext	3.	}Interoperability Quality - the degree to which a service conforms to appropriate standards.\
{\listtext	4.	}Business Processing Quality - the service\'92s reliability for business use considering transmission integrity and integration with other processes.\
{\listtext	5.	}Manageability Quality - management processes to ensure web service quality.\
{\listtext	6.	}Security Quality - the service\'92s ability to prevent intrusion, interception or destruction of the service itself or its messages.\
\pard\tx566\pardeftab720\sl276\slmult1\sa400
\cf0 \expnd0\expndtw0\kerning0
Taken altogether these represent all factors which affect a client\'92s decision to consume a web service. \
The scope of this study is limited to those aspects of Landgate\'92s service that affect their suitability for use on mobile devices. The business process and value, management, interoperability and security factors can not be tested with a mobile device. These are more suited to desktop studies and surveys of existing clients. Only Service Level Measurement Quality is within the purview of this study. \
The sub-categories include;\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl276\slmult1\sa400
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	1.	}Response Time - the time interval between the transmission of a request and the receipt of a response. The total time is composed of the time taken for the client to compose the request and decompose the response, the network transmission time to and from the server and the time taken for the server to process the request and formulate a response.\
{\listtext	2.	}Maximum Throughput - the maximum number of requests a service can reliably respond to in a unit of time.\
{\listtext	3.	}Availability - the proportion of time the server is operational, the complement of service downtime per measured time.\
{\listtext	4.	}Accessibility - the probability of the web service can be reached when the system is operational, quantified as the number of received acknowledgement messages divided by the total number of requests.\
{\listtext	5.	}Successability - the probability of receiving a successful response to a web service request, the number of responses divided by the number of requests.\expnd0\expndtw0\kerning0
\
\pard\tx566\pardeftab720\sl276\slmult1\sa400
\cf0 We propose to track these factors through a series of frequent but irregularly timed tests from a mobile device deployed in situations common to the mobile network milieu.\
Automating web service discovery and quality assessment is a much more active field of research with the aim of supporting semantic web development. An application should be able to dynamically bind the web service without supervision from the end user. How then, though, should the application choose which web service to employ from the multitude available, also without requiring user intervention. The investigators below are proponents of systematically and automatically applied quality metrics as a basis for deciding which web services should be bound.\
Palacios, Garcia-Banjul and Tuya \{*Palacios:2011eo\} surveyed Service Oriented Architecture literature to find articles focussed on dynamic binding. 57% of the 33 articles detected faults in web services and thereby excluded non-responsive services.\
Orion, Marco and French \{*Oriol:2014kq\} reviewed the state of the art in quality of service models for web services, surveying 65 papers written between 2001 and 2012. They showed that most researchers were assessing web services quality in terms of availability (essentially the probability a request will receive a response) with 94% of surveyed papers defining the metric. Response time was second place at 83% coverage and accuracy third with 62%. \
Wu et al \{*Wu:2011kk\} propose web service registries evaluate the quality of services they expose. Service downtime, mismatches between catalogued metadata and current capability or inconsistencies when registered in multiple catalogues can lead to the selection of a suboptimal web mapping service. The catalogue service periodically interrogates an OGC WMS testing all operations listed in its GetCapabilities document. Successful tests decrease the frequency of future tests, while lack of response increases the frequency.\
Wu et al model quality factors using a hierarchical tree, proposed by Hanwu Zhang, one of the authors in their PhD dissertation from 2008 (reference not found in English translation). This is a helpful concept for automated quality analysis as branches in the hierarchy can be weighted differently, emphasising categories of metrics (the \'93leaves\'94) over others. Unfortunately Wu et al\'92s automated analysis stops after comparing the most recent response time to a weighted average of previously recorded response times. Thereafter they assess map data quality through a survey method. A GUI program presents returned data to an expert user for assessment and scoring out of five on a series of quality metrics.\
Miao, Shi and Cao \{*Miao:2011dh\} build upon Wu et al\'92s method of adaptable testing frequency by halving test intervals when the response time is greater than the average of earlier tests. They go further to parameterise the proportion of failed requests. Their process is explained well in a flow chart, omitted from many similar papers. \
Miao, Shi and Cao developed a C based program to crawl 100 WMS servers to measure their performance and stability as per their procedure. The conference paper referenced here omitted a table listing the servers assessed. As with Wu et al, the team leaves data quality assessment to a survey of expert users.\

\f1\b Acceptance
\f0\b0 \
Park and Ohm \{*Park:2014jp\} used survey data to construct a technology acceptance model to investigate users\'92 acceptance of mobile mapping applications. They found that acceptance and hence intention to use a mobile mapping service depended to a large degree upon two factors; perceived locational accuracy and processing speed. Park and Ohm defined perceived locational accuracy as how well users envision their own location in the map, essentially the degree to which mapped features correspond with a user\'92s mental model of the world and where they are in it.\
\
\
BIG DISCUSSION ON;\
WHERE DOES MY WORK FIT IN\'85\
}